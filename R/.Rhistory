plot(rethinking::compare(model_multinetwork_fly, model_multinetwork_run,
model_multinetwork_timeHU, model_multinetwork_distractor,
model_multinetwork_nbsacc, model_multinetwork_statechange,
model_multinetwork_walked, model_multinetwork_target, model_null_Bm_avf))
precis(model_multinetwork_target, depth=2)
setwd("~/ownCloud/bayesian_nbda/STbayes/R")
install.packages("plotrix")
# load required libraries
library(igraph)
library(plotrix)
# define the letters and layout as a network
nodes <- data.frame(
name = c('S1', 'S2', 'T1', 'T2', 'B1', 'B2', 'A1', 'Y1', 'E1', 'S3'),
x = c(1, 2, 4, 5, 8, 9, 11, 13, 15, 17),  # x coordinates for node positions
y = c(3, 2, 3, 2, 3, 2, 3, 3, 3, 3)       # y coordinates for node positions
)
# edges connecting nodes to create a network effect
edges <- data.frame(
from = c('S1', 'S1', 'T1', 'T1', 'B1', 'B1', 'A1', 'Y1', 'E1'),
to   = c('S2', 'T1', 'T2', 'B1', 'B2', 'A1', 'Y1', 'E1', 'S3')
)
# create the graph
graph <- graph_from_data_frame(edges, vertices = nodes, directed = FALSE)
# set PNG export file
png("../data/network_STbayes.png", width = 1200, height = 600)
# plot the network
par(mar = c(0, 0, 0, 0))  # remove margins
plot(
graph,
layout = as.matrix(nodes[, c("x", "y")]),  # fixed node layout
vertex.size = 30,
vertex.label = c('S', 'S', 'T', 'T', 'B', 'B', 'A', 'Y', 'E', 'S'),
vertex.label.cex = 2,
vertex.label.family = "sans",
vertex.label.color = "black",
vertex.color = "white",
edge.width = 3
)
# finalize PNG output
dev.off()
# load required libraries
library(igraph)
library(ggplot2)
# function to generate nodes in the shape of STbayes
get_letter_coords <- function() {
# coordinates for a rough outline of the letters in 'STbayes'
data.frame(
name = 1:50,  # number of nodes
x = c(1, 2, 3, 4, 5, 6, 4, 4, 3, 2, 6, 7, 7, 7, 8, 9, 10, 10, 9, 8, 12, 13, 14, 15, 16, 14, 13, 12, 18, 19, 20, 19, 18, 21, 22, 23, 24, 24, 23, 22, 21, 26, 27, 28, 27, 26, 29, 30, 31, 30),
y = c(8, 8, 8, 8, 8, 8, 7, 6, 5, 4, 4, 4, 5, 6, 7, 8, 8, 7, 6, 5, 8, 8, 8, 8, 8, 7, 6, 5, 8, 8, 8, 7, 6, 4, 4, 4, 4, 5, 6, 7, 8, 8, 8, 8, 7, 6, 4, 4, 4, 5)
)
}
# function to generate edges connecting nearby nodes
get_edges <- function(n) {
edges <- data.frame(from = integer(), to = integer())
for (i in 1:(n - 1)) {
edges <- rbind(edges, data.frame(from = i, to = i + 1))
}
return(edges)
}
# create node and edge data
nodes <- get_letter_coords()
edges <- get_edges(nrow(nodes))
graph <- graph_from_data_frame(edges, vertices = nodes, directed = FALSE)
# export the image
png("network_STbayes.png", width = 1200, height = 600)
par(mar = c(0, 0, 0, 0))
plot(
graph,
layout = as.matrix(nodes[, c("x", "y")]),
vertex.size = 2,
vertex.label = NA,  # no labels on nodes
vertex.color = "black",
edge.width = 1,
edge.color = "gray"
)
dev.off()
# load required libraries
library(igraph)
library(ggplot2)
# function to generate nodes in the shape of STbayes
get_letter_coords <- function() {
# coordinates for a rough outline of the letters in 'STbayes'
data.frame(
name = 1:50,  # number of nodes
x = c(1, 2, 3, 4, 5, 6, 4, 4, 3, 2, 6, 7, 7, 7, 8, 9, 10, 10, 9, 8, 12, 13, 14, 15, 16, 14, 13, 12, 18, 19, 20, 19, 18, 21, 22, 23, 24, 24, 23, 22, 21, 26, 27, 28, 27, 26, 29, 30, 31, 30),
y = c(8, 8, 8, 8, 8, 8, 7, 6, 5, 4, 4, 4, 5, 6, 7, 8, 8, 7, 6, 5, 8, 8, 8, 8, 8, 7, 6, 5, 8, 8, 8, 7, 6, 4, 4, 4, 4, 5, 6, 7, 8, 8, 8, 8, 7, 6, 4, 4, 4, 5)
)
}
# function to generate edges connecting nearby nodes
get_edges <- function(n) {
edges <- data.frame(from = integer(), to = integer())
for (i in 1:(n - 1)) {
edges <- rbind(edges, data.frame(from = i, to = i + 1))
}
return(edges)
}
# create node and edge data
nodes <- get_letter_coords()
edges <- get_edges(nrow(nodes))
graph <- graph_from_data_frame(edges, vertices = nodes, directed = FALSE)
# export the image
png("network_STbayes.png", width = 1200, height = 600)
par(mar = c(0, 0, 0, 0))
plot(
graph,
layout = as.matrix(nodes[, c("x", "y")]),
vertex.size = 2,
vertex.label = NA,  # no labels on nodes
vertex.color = "black",
edge.width = 1,
edge.color = "gray"
)
dev.off()
cat("Image saved as 'network_STbayes.png'\n")
if (!require("devtools")) install.packages("devtools")
devtools::install_github("michaelchimento/STbayes")
library(STbayes)
library(NBDA)
#load example NBDAdata object from Tutorial 4.1 from Hasenjager et al. 2021
nbdaData_cTADA <- STbayes::nbdaData_cTADA
#Fit cTADA model using tadaFit:
model_constant<-NBDA::tadaFit(nbdaData_cTADA)
data.frame(Variable=model_constant@varNames,MLE=model_constant@outputPar,SE=model_constant@se)
library(STbayes)
library(igraph)
library(dplyr)
library(NBDA)
# Parameters
N <- 30  # Population size
k <- 4    # Degree of each node in the random regular graph
lambda_0=0.001 #baseline
A <- 1  # Individual learning rate
s <- 7  # Social learning rate per unit connection
t_steps <- 1000
max_time = t_steps+1
# create random regular graph
g <- sample_k_regular(N, k, directed = FALSE, multiple = FALSE)
V(g)$name
library(STbayes)
library(igraph)
library(dplyr)
library(NBDA)
# Parameters
N <- 30  # Population size
k <- 4    # Degree of each node in the random regular graph
lambda_0=0.001 #baseline
A <- 1  # Individual learning rate
s <- 3  # Social learning rate per unit connection
t_steps <- 1000
max_time = t_steps+1
# create random regular graph
g <- sample_k_regular(N, k, directed = FALSE, multiple = FALSE)
V(g)$name <- 1:N
# initialize a dataframe to store the time of acquisition data
df <- data.frame(id=1:N, time=max_time, max_time = max_time)
# simulate the diffusion
for (t in 1:t_steps) {
# identify knowledgeable individuals
informed <- df[df$time < max_time, "id"]
# identify naive
potential_learners <- c(1:N)
potential_learners <- potential_learners[!(potential_learners %in% informed)]
# break the loop if no one left to learn,
if (length(potential_learners) == 0) break
# calc the hazard
learning_rates <- sapply(potential_learners, function(x) {
neighbors <- neighbors(g, x)
C <- sum(neighbors$name %in% informed)
lambda <- lambda_0 * (A + s*C)
return(lambda)
})
# convert hazard to probability
learning_probs <- 1 - exp(-learning_rates)
# for each potential learner, determine whether they learn the behavior
learners_this_step <- rbinom(n=length(potential_learners), size=1, prob=learning_probs)
# update their time of acquisition
new_learners <- potential_learners[learners_this_step == 1]
df[df$id %in% new_learners, "time"] <- t
}
diffusion_data <- df %>%
arrange(time) %>%
group_by(time, .drop = T) %>%
mutate(tie=ifelse(n()>1,1,0),
seed=ifelse(time==0,1,0))
hist(diffusion_data$time)
# Define the adjacency matrix
adj_matrix <- as_adjacency_matrix(g, attr=NULL, sparse=FALSE)
dim(adj_matrix) = c(N,N,1)
tie_vec = diffusion_data %>% arrange(time) %>% ungroup() %>% select(tie)
seed_vec = diffusion_data %>% arrange(id) %>% ungroup() %>% select(seed)
#### Fit NBDA model ####
d = nbdaData(label="sim_data",
assMatrix = adj_matrix,
orderAcq = diffusion_data$id,
timeAcq = diffusion_data$time,
endTime = max_time,
ties = tie_vec$tie,
demons = seed_vec$seed)
result = tadaFit(d)
data.frame(Variable=result@varNames,MLE=result@outputPar,SE=result@se)
est_rate = 1/result@outputPar[1]
est_rate
#import into STbayes
diffusion_data <- diffusion_data %>%
mutate(trial=1) %>%
select(-c(tie, seed))
edge_list <- as.data.frame(as_edgelist(g))
names(edge_list) = c("from","to")
edge_list$trial = 1
edge_list$assoc = 1 #assign named edgeweight since this is just an edge list
#generate STAN model from input data
data_list_user = import_user_STb(diffusion_data, edge_list)
#generate STAN model from input data
model_obj = generate_STb_model(data_list_user)
# fit model
fit = fit_STb(data_list_user, "../inst/stan/model_from_simulate_data.stan", chains = 5, cores = 5, iter=2000, control = list(adapt_delta=0.99))
# check estimates
STb_summary(fit, digits=4)
#' @param ignore_params character vector of parameters to ignore
#' @param digits integer of digits to round to
#'
#' @return Summary table
#' @importFrom rstan extract Rhat
#' @importFrom coda as.mcmc HPDinterval effectiveSize
#' @export
#'
#' @examples
#' summary_table <- STb_summary(fit)
STb_summary <- function(fit, depth = 1, prob = 0.95, ignore_params = c("s", "lambda_0", "lp__"), digits = 3) {
# extract posterior samples
samples <- rstan::extract(fit, permuted = TRUE)
# filter by depth and ignore specific parameters
param_names <- names(samples)
param_depths <- sapply(strsplit(param_names, "\\["), function(x) length(x))
selected_params <- param_names[param_depths <= depth & !param_names %in% ignore_params]
# add transformed parameters
transformed_params <- list(
transformed_s = exp(samples$log_s_mean),
transformed_baserate = 1 / exp(samples$log_lambda_0_mean)
)
summary_table <- data.frame(
Parameter = character(),
Median = numeric(),
HPDI_Lower = numeric(),
HPDI_Upper = numeric(),
n_eff = numeric(),
Rhat = numeric(),
stringsAsFactors = FALSE
)
# process each parameter
for (param in selected_params) {
param_samples <- samples[[param]]
param_samples <- coda::as.mcmc(as.matrix(param_samples))
# compute statistics
mean_value <- mean(param_samples)
median_value <- median(param_samples)
hpdi <- coda::HPDinterval(param_samples, prob = prob)
n_eff <- coda::effectiveSize(param_samples)
rhat <- rstan::Rhat(as.matrix(param_samples))
# append
summary_table <- rbind(summary_table, data.frame(
Parameter = param,
Mean = mean_value,
Median = median_value,
HPDI_Lower = hpdi[1, "lower"],
HPDI_Upper = hpdi[1, "upper"],
n_eff = n_eff,
Rhat = rhat,
stringsAsFactors = FALSE
))
}
for (param in names(transformed_params)) {
param_samples <- transformed_params[[param]]
mean_value <- mean(param_samples)
median_value <- median(param_samples)
hpdi <- coda::HPDinterval(param_samples, prob = prob)
n_eff <- coda::effectiveSize(param_samples)
rhat <- rstan::Rhat(as.matrix(param_samples))
summary_table <- rbind(summary_table, data.frame(
Parameter = param,
Mean = mean_value,
Median = median_value,
HPDI_Lower = hpdi[1, "lower"],
HPDI_Upper = hpdi[1, "upper"],
n_eff = n_eff,
Rhat = rhat,
stringsAsFactors = FALSE
))
}
row.names(summary_table) = NULL
numeric_cols <- sapply(summary_table, is.numeric)
summary_table[numeric_cols] <- lapply(summary_table[numeric_cols], round, digits = digits)
return(summary_table)
}
# check estimates
STb_summary(fit, digits=4)
#' @param ignore_params character vector of parameters to ignore
#' @param digits integer of digits to round to
#'
#' @return Summary table
#' @importFrom rstan extract Rhat
#' @importFrom coda as.mcmc HPDinterval effectiveSize
#' @export
#'
#' @examples
#' summary_table <- STb_summary(fit)
STb_summary <- function(fit, depth = 1, prob = 0.95, ignore_params = c("s", "lambda_0", "lp__"), digits = 3) {
# extract posterior samples
samples <- rstan::extract(fit, permuted = TRUE)
# filter by depth and ignore specific parameters
param_names <- names(samples)
param_depths <- sapply(strsplit(param_names, "\\["), function(x) length(x))
selected_params <- param_names[param_depths <= depth & !param_names %in% ignore_params]
# add transformed parameters
transformed_params <- list(
transformed_s = exp(samples$log_s_mean),
transformed_baserate = 1 / exp(samples$log_lambda_0_mean)
)
summary_table <- data.frame(
Parameter = character(),
Median = numeric(),
HPDI_Lower = numeric(),
HPDI_Upper = numeric(),
n_eff = numeric(),
Rhat = numeric(),
stringsAsFactors = FALSE
)
# process each parameter
for (param in selected_params) {
param_samples <- samples[[param]]
param_samples <- coda::as.mcmc(as.matrix(param_samples))
# compute statistics
mean_value <- mean(param_samples)
median_value <- median(param_samples)
hpdi <- coda::HPDinterval(param_samples, prob = prob)
n_eff <- coda::effectiveSize(param_samples)
rhat <- rstan::Rhat(as.matrix(param_samples))
# append
summary_table <- rbind(summary_table, data.frame(
Parameter = param,
Mean = mean_value,
Median = median_value,
HPDI_Lower = hpdi[1, "lower"],
HPDI_Upper = hpdi[1, "upper"],
n_eff = n_eff,
Rhat = rhat,
stringsAsFactors = FALSE
))
}
for (param in names(transformed_params)) {
param_samples <- transformed_params[[param]]
param_samples <- coda::as.mcmc(as.matrix(param_samples))
mean_value <- mean(param_samples)
median_value <- median(param_samples)
hpdi <- coda::HPDinterval(param_samples, prob = prob)
n_eff <- coda::effectiveSize(param_samples)
rhat <- rstan::Rhat(as.matrix(param_samples))
summary_table <- rbind(summary_table, data.frame(
Parameter = param,
Mean = mean_value,
Median = median_value,
HPDI_Lower = hpdi[1, "lower"],
HPDI_Upper = hpdi[1, "upper"],
n_eff = n_eff,
Rhat = rhat,
stringsAsFactors = FALSE
))
}
row.names(summary_table) = NULL
numeric_cols <- sapply(summary_table, is.numeric)
summary_table[numeric_cols] <- lapply(summary_table[numeric_cols], round, digits = digits)
return(summary_table)
}
# check estimates
STb_summary(fit, digits=4)
# Parameters
N <- 50  # Population size
k <- 4    # Degree of each node in the random regular graph
lambda_0=0.001 #baseline
A <- 1  # Individual learning rate
s <- 7  # Social learning rate per unit connection
t_steps <- 1000
max_time = t_steps+1
# create random regular graph
g <- sample_k_regular(N, k, directed = FALSE, multiple = FALSE)
V(g)$name <- 1:N
# initialize a dataframe to store the time of acquisition data
df <- data.frame(id=1:N, time=max_time, max_time = max_time)
# simulate the diffusion
for (t in 1:t_steps) {
# identify knowledgeable individuals
informed <- df[df$time < max_time, "id"]
# identify naive
potential_learners <- c(1:N)
potential_learners <- potential_learners[!(potential_learners %in% informed)]
# break the loop if no one left to learn,
if (length(potential_learners) == 0) break
# calc the hazard
learning_rates <- sapply(potential_learners, function(x) {
neighbors <- neighbors(g, x)
C <- sum(neighbors$name %in% informed)
lambda <- lambda_0 * (A + s*C)
return(lambda)
})
# convert hazard to probability
learning_probs <- 1 - exp(-learning_rates)
# for each potential learner, determine whether they learn the behavior
learners_this_step <- rbinom(n=length(potential_learners), size=1, prob=learning_probs)
# update their time of acquisition
new_learners <- potential_learners[learners_this_step == 1]
df[df$id %in% new_learners, "time"] <- t
}
diffusion_data <- df %>%
arrange(time) %>%
group_by(time, .drop = T) %>%
mutate(tie=ifelse(n()>1,1,0),
seed=ifelse(time==0,1,0))
hist(diffusion_data$time)
# Define the adjacency matrix
adj_matrix <- as_adjacency_matrix(g, attr=NULL, sparse=FALSE)
dim(adj_matrix) = c(N,N,1)
tie_vec = diffusion_data %>% arrange(time) %>% ungroup() %>% select(tie)
seed_vec = diffusion_data %>% arrange(id) %>% ungroup() %>% select(seed)
#### Fit NBDA model ####
d = nbdaData(label="sim_data",
assMatrix = adj_matrix,
orderAcq = diffusion_data$id,
timeAcq = diffusion_data$time,
endTime = max_time,
ties = tie_vec$tie,
demons = seed_vec$seed)
result = tadaFit(d)
data.frame(Variable=result@varNames,MLE=result@outputPar,SE=result@se)
# Parameters
N <- 50  # Population size
k <- 4    # Degree of each node in the random regular graph
lambda_0=0.001 #baseline
A <- 1  # Individual learning rate
s <- 7  # Social learning rate per unit connection
t_steps <- 1000
max_time = t_steps+1
# create random regular graph
g <- sample_k_regular(N, k, directed = FALSE, multiple = FALSE)
V(g)$name <- 1:N
# initialize a dataframe to store the time of acquisition data
df <- data.frame(id=1:N, time=max_time, max_time = max_time)
# simulate the diffusion
for (t in 1:t_steps) {
# identify knowledgeable individuals
informed <- df[df$time < max_time, "id"]
# identify naive
potential_learners <- c(1:N)
potential_learners <- potential_learners[!(potential_learners %in% informed)]
# break the loop if no one left to learn,
if (length(potential_learners) == 0) break
# calc the hazard
learning_rates <- sapply(potential_learners, function(x) {
neighbors <- neighbors(g, x)
C <- sum(neighbors$name %in% informed)
lambda <- lambda_0 * (A + s*C)
return(lambda)
})
# convert hazard to probability
learning_probs <- 1 - exp(-learning_rates)
# for each potential learner, determine whether they learn the behavior
learners_this_step <- rbinom(n=length(potential_learners), size=1, prob=learning_probs)
# update their time of acquisition
new_learners <- potential_learners[learners_this_step == 1]
df[df$id %in% new_learners, "time"] <- t
}
diffusion_data <- df %>%
arrange(time) %>%
group_by(time, .drop = T) %>%
mutate(tie=ifelse(n()>1,1,0),
seed=ifelse(time==0,1,0))
hist(diffusion_data$time)
# Define the adjacency matrix
adj_matrix <- as_adjacency_matrix(g, attr=NULL, sparse=FALSE)
dim(adj_matrix) = c(N,N,1)
tie_vec = diffusion_data %>% arrange(time) %>% ungroup() %>% select(tie)
seed_vec = diffusion_data %>% arrange(id) %>% ungroup() %>% select(seed)
#### Fit NBDA model ####
d = nbdaData(label="sim_data",
assMatrix = adj_matrix,
orderAcq = diffusion_data$id,
timeAcq = diffusion_data$time,
endTime = max_time,
ties = tie_vec$tie,
demons = seed_vec$seed)
result = tadaFit(d)
data.frame(Variable=result@varNames,MLE=result@outputPar,SE=result@se)
est_rate = 1/result@outputPar[1]
est_rate
#import into STbayes
diffusion_data <- diffusion_data %>%
mutate(trial=1) %>%
select(-c(tie, seed))
edge_list <- as.data.frame(as_edgelist(g))
names(edge_list) = c("from","to")
edge_list$trial = 1
edge_list$assoc = 1 #assign named edgeweight since this is just an edge list
#generate STAN model from input data
data_list_user = import_user_STb(diffusion_data, edge_list)
#generate STAN model from input data
model_obj = generate_STb_model(data_list_user)
# fit model
fit = fit_STb(data_list_user, "../inst/stan/model_from_simulate_data.stan", chains = 5, cores = 5, iter=2000, control = list(adapt_delta=0.99))
# suggest writing to file for debugging
write(model_obj, file = "../inst/stan/model_from_simulate_data.stan")
# check estimates
STb_summary(fit, digits=4)

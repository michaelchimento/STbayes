---
title: "Getting started with STbayes"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting started with STbayes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Load the package and import event and network data from 1 simulated trial.
```{r setup}
library(STbayes)

event_data <- STbayes::event_data
head(event_data)

edge_list <- STbayes::edge_list
head(edge_list)
```

The first step is to import the data into a format that can be used with other STbayes functions. I've tried to abstract away as much complexity as possible, but please check the function arguments before sending in the data. The default values might not be what you want! In this case, it's simple---we have no covariates and a single trial with a static network.

```{r import}
data_list <- import_user_STb(event_data, edge_list)
```

This function does quite a bit behind the scenes to process the raw observational data into a named list of variables. I've tried to optimize as much as possible so it shouldn't take more than a second or two unless you're using gigantic networks. Important bits include:

1. *Validate and standardize inputs.* The function confirms that necessary columns are present, and maps individual and trial identifiers across data frames. The IDs present in the network data are used as the reference for any other data that you supply. For example, if an ID is in the edge list, but not in the event data, it will be automatically added as a censored individual into the event_data. We strongly suggest accounting for all individuals in all data provided to this function to avoid any surprises later on in the analysis.
2. *Preprocess event data.* The function sorts events by time within each trial and assigns a discrete time index that is used to refer to inter-event intervals. E.g. ```timestep=1``` corresponds to the start of the observation period until the first event. If ```high-res``` mode is used, all durations are set to 1. It also calculates the duration of each of these inter-event intervals. It identifies demonstrators (```time==0```) and right censored individuals (```time>t_end```).
3. *Preprocess network data.* If static, a single matrix is used for all times. If dynamic, it aligns edge weights across trials and inter-event intervals. If high resolution, it will collapse data into inter-event intervals, multiplying edge weights by transmission weights and then dividing by duration. When fitting, this term will be multiplied out again by duration, so for linear transmission, this is equivalent to running the likelihood in Stan for every single time-step and greatly speeds up model fitting. We note that this pre-processing is not possible complex transmission models.
4. *Preprocess covariates.* The function recognizes and stores the roles of ILVs as affecting (or not) the intrinsic rate, the social rate or both rates (multiplicative).

Next, we can pass this into a function that automatically creates Stan code that is customised to your data and modelling desires. Each time you generate a model, the function will display the default priors it'll use. You can specify your own priors using the argument ```priors``` and give it a named list.

```{r generate_model}
model_obj <- generate_STb_model(data_list, gq = T)
```

This returns a very long string containing the code. The argument ```gq``` indicates whether a generated quantities block will be created. This defaults to TRUE, as it's needed to output the log-likelihood of observations for any kind of model comparison later. To view it in R, just use ```cat()```. Otherwise, I recommend saving it, as the formatting might need a bit of cleaning before it's readable:

```{r save_model, eval=F}
write(model_obj, file="../data/stan_models/my_first_model.stan")
```

If you want to directly modify the code, go ahead. You can supply the file path of the model rather than ```model_obj``` in the next call to ```fit_STb()```. Here, we'll just use the model stored in the variable. We supply the data_list and model_obj for fitting the model:

```{r fit}
full_fit <- fit_STb(data_list,
    model_obj,
    parallel_chains = 4,
    chains = 4,
    cores = 4,
    iter = 4000,
    refresh=1000
)
```

This more or less acts as a wrapper for calling cmdstans $sample(), and you can add any extra arguments that would be valid. Here, I've made sure that we run 4 chains in parallel for 4000 iterations, and only to update us on progress every 1000 iterations. By default, it will do iter/2 worth of warmup, and iter/2 worth of sampling. Fitting can take anywhere from seconds to hours depending on your computer and the amount of data. Fitting this dataset _should_ be quick. If it's very slow, you might want to try running this on your friend's computer. I've tried to make things reasonably efficient, but have not parallelized the likelihood yet.

I strongly recommend using the convenience function below to save the output after fitting. If you only save the fit object to an rda using something like ```save()```, it will not include the data output from chains, making it useless. ```STb_save``` saves these alongside the fit in a single file:

```{r save, eval=F}
STb_save(full_fit, output_dir = "cmdstan_saves", name="my_first_fit")
#> Fit & chains successfully saved ðŸ’¾ 
#> You can load fit again with ðŸ‘‰ readRDS('cmdstan_saves/my_first_fit.rds') 
```

If you don't provide a name, it will use the name of the object. 

Finally, you can inspect the parameter estimates using the convenience function below:

```{r summary}
STb_summary(full_fit, digits = 3)
```

Note that parameters are estimated on the log scale, but converted to linear scale in the generated quantities. lambda_0 and s are the same interpretation as the NBDA package. Keep in mind s_prime = s * lambda_0. It usually isn't reported so I exclude it's linear transformation. percent_ST[1] is the estimated percentage of events that occurred through social transmission. The [1] refers to network 1, as we've only given the model one network. If you fit a multi-network model, all networks will have an estimate.

This is the basics of using STbayes. You only need event data and network data for a minimal model. However, the package is very flexible and you can make use of many other types of data and formulations.

At this point, you are just dealing with a cmdstan fit, so if you have your own pipeline, adios! However, I have provided a few more convenience functions detailed in the other vignettes for model comparison, and posterior predictive checks.
